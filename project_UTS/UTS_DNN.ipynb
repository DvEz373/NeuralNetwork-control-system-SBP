{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qOklCj1kUh6J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aDlR33OGUh6L"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the number of samples\n",
        "k_max = 50000\n",
        "t = np.linspace(0, k_max, k_max)  # Time variable\n",
        "\n",
        "# Initialize u(k) and y(k) arrays with dtype float32\n",
        "u = np.zeros(k_max, dtype=np.float32)\n",
        "y = np.zeros(k_max, dtype=np.float32)\n",
        "\n",
        "# Generate random inputs (float32)\n",
        "u = (20 * np.random.random(k_max) - 10).astype(np.float32)\n",
        "\n",
        "# Define y(k) calculations with dtype float32\n",
        "for k in range(1, k_max):\n",
        "    y[k] = np.float32(1 / (1 + (y[k-1])**2)) + np.float32(0.25 * u[k]) - np.float32(0.3 * u[k-1])\n",
        "\n",
        "# Shift arrays for y(k-1), y(k-2), u(k-1), u(k-2)\n",
        "y_k_1 = np.zeros(k_max, dtype=np.float32)\n",
        "y_k_2 = np.zeros(k_max, dtype=np.float32)\n",
        "u_k_1 = np.zeros(k_max, dtype=np.float32)\n",
        "u_k_2 = np.zeros(k_max, dtype=np.float32)\n",
        "\n",
        "y_k_1[1:] = y[:-1]\n",
        "y_k_2[2:] = y[:-2]\n",
        "u_k_1[1:] = u[:-1]\n",
        "u_k_2[2:] = u[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "moVg6NlyUh6M",
        "outputId": "871e2f64-a396-4c0b-8da5-872f8da9c82b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(t, u, label='u(k)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('u(k)')\n",
        "plt.title('Plot of u(k) over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(t, y, label='y(k)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('y(k)')\n",
        "plt.title('Plot of y(k) over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SlcMc9SZUh6O"
      },
      "outputs": [],
      "source": [
        "def normalize(data):\n",
        "    data = data.astype(np.float32)\n",
        "    return 2 * (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0)) - 1\n",
        "\n",
        "def denormalize(normalized_data, data_min, data_max):\n",
        "    normalized_data = normalized_data.astype(np.float32)\n",
        "    return (normalized_data + 1) * (data_max - data_min) / 2 + data_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kowuwOvRUh6Q"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "split_ratio = 0.5\n",
        "split_index = int(k_max * split_ratio)\n",
        "\n",
        "X = np.array([y_k_1, y_k_2, u, u_k_1, u_k_2]).T\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "y_train, y_val = y[:split_index], y[split_index:]\n",
        "\n",
        "# Normalize inputs and outputs separately\n",
        "u_norm = normalize(u)            # Normalize input u(k)\n",
        "u_k_1_norm = normalize(u_k_1)    # Normalize u(k-1)\n",
        "u_k_2_norm = normalize(u_k_2)    # Normalize u(k-2)\n",
        "\n",
        "# Normalize and reshape y(k), y(k-1), and y(k-2)\n",
        "y_norm = normalize(y).reshape(-1, 1)            # Normalize output y(k) and reshape to (k_max, 1)\n",
        "y_k_1_norm = normalize(y_k_1).reshape(-1, 1)    # Normalize y(k-1) and reshape to (k_max, 1)\n",
        "y_k_2_norm = normalize(y_k_2).reshape(-1, 1)    # Normalize y(k-2) and reshape to (k_max, 1)\n",
        "\n",
        "# Combine normalized inputs into X (features matrix), no need to reshape X\n",
        "X_norm = np.array([y_k_1_norm.flatten(), y_k_2_norm.flatten(), u_norm, u_k_1_norm, u_k_2_norm], dtype=np.float32).T\n",
        "\n",
        "# Split the normalized data into training and validation sets\n",
        "split_index = int(k_max * split_ratio)\n",
        "\n",
        "X_train_norm = X_norm[:split_index]\n",
        "X_val_norm = X_norm[split_index:]\n",
        "\n",
        "y_train_norm = y_norm[:split_index]\n",
        "y_val_norm = y_norm[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01, l2_lambda=0.001):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda  # L2 regularization parameter\n",
        "\n",
        "        # Initialize weights and biases using Nguyen-Widrow initialization\n",
        "        self.W = []\n",
        "        self.b = []\n",
        "\n",
        "        # Layer 1 (input to first hidden layer)\n",
        "        self.W.append(self.nguyen_widrow_init(input_size, hidden_sizes[0]))\n",
        "        self.b.append(np.zeros((1, hidden_sizes[0]), dtype=np.float32))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.W.append(self.nguyen_widrow_init(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "            self.b.append(np.zeros((1, hidden_sizes[i]), dtype=np.float32))\n",
        "\n",
        "        # Output layer (last hidden layer to output)\n",
        "        self.W.append(self.nguyen_widrow_init(hidden_sizes[-1], output_size))\n",
        "        self.b.append(np.zeros((1, output_size), dtype=np.float32))\n",
        "\n",
        "    def nguyen_widrow_init(self, input_size, output_size):\n",
        "        \"\"\"Nguyen-Widrow initialization.\"\"\"\n",
        "        W = np.random.randn(input_size, output_size).astype(np.float32) * 0.5\n",
        "        beta = 0.7 * (output_size ** (1.0 / input_size))\n",
        "        norm = np.linalg.norm(W, axis=0)\n",
        "        return beta * W / norm\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def mse_loss(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    def r2_score(self, y_true, y_pred):\n",
        "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2) + 1e-4  # Prevent division by zero\n",
        "        return 1 - (ss_res / ss_tot)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass through all layers\n",
        "        self.activations = []  # Store activations for backpropagation\n",
        "        self.Z = []  # Store pre-activation values\n",
        "\n",
        "        # Input to first hidden layer\n",
        "        Z = np.dot(X, self.W[0]) + self.b[0]\n",
        "        A = self.tanh(Z)\n",
        "        self.Z.append(Z)\n",
        "        self.activations.append(A)\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(self.hidden_sizes)):\n",
        "            Z = np.dot(self.activations[-1], self.W[i]) + self.b[i]\n",
        "            A = self.tanh(Z)\n",
        "            self.Z.append(Z)\n",
        "            self.activations.append(A)\n",
        "\n",
        "        # Apply tanh at the output layer (for control system constraint)\n",
        "        Z = np.dot(self.activations[-1], self.W[-1]) + self.b[-1]\n",
        "        A = self.tanh(Z)\n",
        "        self.Z.append(Z)\n",
        "        self.activations.append(A)\n",
        "\n",
        "        return A  # Final output after tanh activation\n",
        "\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]  # Number of training samples\n",
        "\n",
        "        # Gradient for output layer\n",
        "        dZ = (y_pred - y_true) * self.tanh_derivative(self.Z[-1])  # Gradient of output layer with tanh derivative\n",
        "        dW = np.dot(self.activations[-2].T, dZ) / m + (self.l2_lambda * self.W[-1] / m)  # L2 regularization\n",
        "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases for output layer\n",
        "        self.W[-1] -= self.learning_rate * dW\n",
        "        self.b[-1] -= self.learning_rate * db\n",
        "\n",
        "        # Backpropagate through the hidden layers\n",
        "        for i in range(len(self.hidden_sizes) - 1, 0, -1):\n",
        "            dA = np.dot(dZ, self.W[i + 1].T)\n",
        "            dZ = dA * self.tanh_derivative(self.Z[i])\n",
        "            dW = np.dot(self.activations[i - 1].T, dZ) / m + (self.l2_lambda * self.W[i] / m)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "            self.W[i] -= self.learning_rate * dW\n",
        "            self.b[i] -= self.learning_rate * db\n",
        "\n",
        "        # Gradient for the first hidden layer\n",
        "        dA = np.dot(dZ, self.W[1].T)\n",
        "        dZ = dA * self.tanh_derivative(self.Z[0])\n",
        "        dW = np.dot(X.T, dZ) / m + (self.l2_lambda * self.W[0] / m)\n",
        "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases for the first hidden layer\n",
        "        self.W[0] -= self.learning_rate * dW\n",
        "        self.b[0] -= self.learning_rate * db\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
        "        # Create attributes for the losses and accuracies to be accessed later\n",
        "        self.train_losses, self.val_losses = [], []\n",
        "        self.train_r2_scores, self.val_r2_scores = [], []\n",
        "\n",
        "        # Store predictions as attributes\n",
        "        self.y_train_pred = None  # To store training predictions\n",
        "        self.y_val_pred = None  # To store validation predictions\n",
        "\n",
        "        print(f\"{'Epoch':<10}{'Train Loss':<15}{'Train R²':<15}{'Val Loss':<15}{'Val R²':<15}\")\n",
        "        print(\"=\" * 65)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass (training set)\n",
        "            self.y_train_pred = self.forward(X_train)  # Store training predictions\n",
        "\n",
        "            # Compute training loss and R² score\n",
        "            train_loss = self.mse_loss(y_train, self.y_train_pred)\n",
        "            train_r2 = self.r2_score(y_train, self.y_train_pred)\n",
        "\n",
        "            # Forward pass (validation set)\n",
        "            self.y_val_pred = self.forward(X_val)  # Store validation predictions\n",
        "\n",
        "            # Compute validation loss and R² score\n",
        "            val_loss = self.mse_loss(y_val, self.y_val_pred)\n",
        "            val_r2 = self.r2_score(y_val, self.y_val_pred)\n",
        "\n",
        "            # Store metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_r2_scores.append(train_r2)\n",
        "            self.val_r2_scores.append(val_r2)\n",
        "\n",
        "            # Backward pass (training set)\n",
        "            self.backward(X_train, y_train, self.y_train_pred)\n",
        "\n",
        "            # Print progress every 100 epochs\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"{epoch + 1:<10}{train_loss:<15.4f}{train_r2:<15.4f}{val_loss:<15.4f}{val_r2:<15.4f}\")\n",
        "\n",
        "        print(\"=\" * 65)\n",
        "        print(f\"Final Training Loss: {train_loss:.4f}, Final Training R²: {train_r2:.4f}\")\n",
        "        print(f\"Final Validation Loss: {val_loss:.4f}, Final Validation R²: {val_r2:.4f}\")\n",
        "\n",
        "        # After training, plot the metrics\n",
        "        self.plot_training_history(self.train_losses, self.val_losses, self.train_r2_scores, self.val_r2_scores)\n",
        "\n",
        "    def plot_training_history(self, train_losses, val_losses, train_r2_scores, val_r2_scores):\n",
        "        epochs_range = range(1, len(train_losses) + 1)\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss (MSE)\")\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.legend()\n",
        "\n",
        "        # R² Scores plot (Accuracy)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_range, train_r2_scores, label=\"Training R² Score\")\n",
        "        plt.plot(epochs_range, val_r2_scores, label=\"Validation R² Score\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"R² Score\")\n",
        "        plt.title(\"Training and Validation R² Score\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the neural network\n",
        "input_size = 5\n",
        "hidden_sizes = [7, 7, 7, 7, 7]  # Five hidden layers with 7 neurons each\n",
        "output_size = 1\n",
        "learning_rate = 0.1  # Reduced learning rate for stability\n",
        "l2_lambda = 0.1  # Regularization parameter\n",
        "epochs = 5000\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_sizes, output_size, learning_rate, l2_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the network and print the losses and R² scores\n",
        "nn.train(X_train_norm, y_train_norm, X_val_norm, y_val_norm, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the stored losses and accuracies\n",
        "train_losses = nn.train_losses  # List of training losses\n",
        "val_losses = nn.val_losses      # List of validation losses\n",
        "train_r2_scores = nn.train_r2_scores  # List of training R² scores\n",
        "val_r2_scores = nn.val_r2_scores      # List of validation R² scores\n",
        "\n",
        "# Print the final losses and R² scores\n",
        "print(\"Final Training Losses:\", train_losses[-1])\n",
        "print(\"Final Validation Losses:\", val_losses[-1])\n",
        "print(\"Final Training R² Scores:\", train_r2_scores[-1])\n",
        "print(\"Final Validation R² Scores:\", val_r2_scores[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn.plot_training_history(train_losses, val_losses, train_r2_scores, val_r2_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_actual_vs_predicted(y_actual, y_pred, title=\"Actual vs Predicted\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot actual values\n",
        "    plt.plot(y_actual, label=\"Actual\", color=\"blue\")\n",
        "    \n",
        "    # Plot predicted values\n",
        "    plt.plot(y_pred, label=\"Predicted\", color=\"orange\", linestyle='dashed')\n",
        "    \n",
        "    # Labels and title\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Output (y)\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the predictions for the training and validation sets\n",
        "y_train_pred_norm = nn.y_train_pred  # Predicted values for the training set\n",
        "y_val_pred_norm = nn.y_val_pred      # Predicted values for the validation set\n",
        "\n",
        "# Denormalize the predictions\n",
        "y_train_pred = denormalize(y_train_pred_norm, np.min(y_train), np.max(y_train))\n",
        "y_val_pred = denormalize(y_val_pred_norm, np.min(y_val), np.max(y_val))\n",
        "\n",
        "# Call the function to plot for training and validation\n",
        "plot_actual_vs_predicted(y_train, y_train_pred, title=\"Training: Actual vs Predicted\")\n",
        "plot_actual_vs_predicted(y_val, y_val_pred, title=\"Validation: Actual vs Predicted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access weights and biases after training\n",
        "weights = nn.W  # List of weight matrices for each layer\n",
        "biases = nn.b   # List of bias vectors for each layer\n",
        "\n",
        "# Print weights and biases for each layer\n",
        "for i, (W, b) in enumerate(zip(weights, biases)):\n",
        "    print(f\"Layer {i + 1} Weights (W{i+1}):\")\n",
        "    print(W)\n",
        "    print(f\"Layer {i + 1} Biases (b{i+1}):\")\n",
        "    print(b)\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetworkStochastic:\n",
        "    def __init__(self, W, b, learning_rate=0.0001, l2_lambda=0.001, clip_value=5):\n",
        "        self.W = W  # Predefined weights\n",
        "        self.b = b  # Predefined biases\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda  # L2 regularization parameter\n",
        "        self.clip_value = clip_value  # For gradient clipping\n",
        "\n",
        "        # Extract hidden sizes from the provided weight dimensions\n",
        "        self.hidden_sizes = [W[i].shape[1] for i in range(len(W) - 1)]\n",
        "        self.output_size = W[-1].shape[1]\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def mse_loss(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    def r2_score(self, y_true, y_pred):\n",
        "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2) + 1e-4  # Avoid division by zero\n",
        "        return 1 - (ss_res / ss_tot)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass through all layers\n",
        "        self.activations = []  # Store activations for backpropagation\n",
        "        self.Z = []  # Store pre-activation values\n",
        "\n",
        "        # Input to first hidden layer\n",
        "        Z = np.dot(X, self.W[0]) + self.b[0]\n",
        "        A = self.tanh(Z)\n",
        "        self.Z.append(Z)\n",
        "        self.activations.append(A)\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(self.hidden_sizes)):\n",
        "            Z = np.dot(self.activations[-1], self.W[i]) + self.b[i]\n",
        "            A = self.tanh(Z)\n",
        "            self.Z.append(Z)\n",
        "            self.activations.append(A)\n",
        "\n",
        "        # Output layer (tanh activation for control system constraint)\n",
        "        Z = np.dot(self.activations[-1], self.W[-1]) + self.b[-1]\n",
        "        self.Z.append(Z)\n",
        "        A = self.tanh(Z)  # Apply tanh activation in the output layer\n",
        "        self.activations.append(A)\n",
        "\n",
        "        return A  # Final output with tanh activation\n",
        "\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        # Single data point (stochastic gradient descent)\n",
        "        m = 1  # Only one data point for SGD\n",
        "\n",
        "        # Gradient for output layer\n",
        "        dZ = (y_pred - y_true) * self.tanh_derivative(self.Z[-1])  # Derivative of tanh at the output\n",
        "        dW = np.dot(self.activations[-2].T, dZ) / m + (self.l2_lambda * self.W[-1] / m)\n",
        "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradient clipping\n",
        "        dW = np.clip(dW, -self.clip_value, self.clip_value)\n",
        "        db = np.clip(db, -self.clip_value, self.clip_value)\n",
        "\n",
        "        # Update weights and biases for output layer\n",
        "        self.W[-1] -= self.learning_rate * dW\n",
        "        self.b[-1] -= self.learning_rate * db\n",
        "\n",
        "        # Backpropagate through the hidden layers\n",
        "        for i in range(len(self.hidden_sizes) - 1, 0, -1):\n",
        "            dA = np.dot(dZ, self.W[i + 1].T)\n",
        "            dZ = dA * self.tanh_derivative(self.Z[i])\n",
        "            dW = np.dot(self.activations[i - 1].T, dZ) / m + (self.l2_lambda * self.W[i] / m)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "            # Gradient clipping\n",
        "            dW = np.clip(dW, -self.clip_value, self.clip_value)\n",
        "            db = np.clip(db, -self.clip_value, self.clip_value)\n",
        "\n",
        "            self.W[i] -= self.learning_rate * dW\n",
        "            self.b[i] -= self.learning_rate * db\n",
        "\n",
        "        # Gradient for the first hidden layer\n",
        "        dA = np.dot(dZ, self.W[1].T)\n",
        "        dZ = dA * self.tanh_derivative(self.Z[0])\n",
        "        dW = np.dot(X.T, dZ) / m + (self.l2_lambda * self.W[0] / m)\n",
        "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradient clipping\n",
        "        dW = np.clip(dW, -self.clip_value, self.clip_value)\n",
        "        db = np.clip(db, -self.clip_value, self.clip_value)\n",
        "\n",
        "        # Update weights and biases for the first hidden layer\n",
        "        self.W[0] -= self.learning_rate * dW\n",
        "        self.b[0] -= self.learning_rate * db\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
        "        # Initialize previous predictions for the first two iterations\n",
        "        y_pred_k_1 = y_train[-1, 0]\n",
        "        y_pred_k_2 = y_train[-2, 0]\n",
        "\n",
        "        # Create attributes for the losses and accuracies to be accessed later\n",
        "        self.train_losses, self.val_losses = [], []\n",
        "        self.train_r2_scores, self.val_r2_scores = [], []\n",
        "\n",
        "        # Store predictions as attributes\n",
        "        self.y_train_pred = None  # To store training predictions\n",
        "        self.y_val_pred = None  # To store validation predictions\n",
        "\n",
        "        print(f\"{'Epoch':<10}{'Train Loss':<15}{'Train R²':<15}{'Val Loss':<15}{'Val R²':<15}\")\n",
        "        print(\"=\" * 65)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # Training process (stochastic)\n",
        "            for i in range(X_train.shape[0]):\n",
        "                # For the first two data points, use previous known y values\n",
        "                if i < 2:\n",
        "                    X_input = np.array([X_train[i, 0], X_train[i, 1], X_train[i, 2], y_pred_k_1, y_pred_k_2]).reshape(1, -1)\n",
        "                else:\n",
        "                    # Use the predictions from previous steps\n",
        "                    X_input = np.array([X_train[i, 0], X_train[i, 1], X_train[i, 2], y_pred_k_1, y_pred_k_2]).reshape(1, -1)\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(X_input)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.mse_loss(y_train[i:i+1], y_pred)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(X_input, y_train[i:i+1], y_pred)\n",
        "\n",
        "                # Update previous predictions\n",
        "                y_pred_k_2 = y_pred_k_1\n",
        "                y_pred_k_1 = y_pred[0, 0]\n",
        "\n",
        "            # Calculate average loss for training\n",
        "            train_loss = epoch_loss / X_train.shape[0]\n",
        "\n",
        "            # Validation pass\n",
        "            self.y_val_pred = self.forward(X_val)  # Store validation predictions\n",
        "            val_loss = self.mse_loss(y_val, self.y_val_pred)\n",
        "            val_r2 = self.r2_score(y_val, self.y_val_pred)\n",
        "\n",
        "            # Calculate R² for training data\n",
        "            self.y_train_pred = self.forward(X_train)  # Store training predictions\n",
        "            train_r2 = self.r2_score(y_train, self.y_train_pred)\n",
        "\n",
        "            # Store metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_r2_scores.append(train_r2)\n",
        "            self.val_r2_scores.append(val_r2)\n",
        "\n",
        "            # Print metrics for the current epoch\n",
        "            print(f\"{epoch + 1:<10}{train_loss:<15.4f}{train_r2:<15.4f}{val_loss:<15.4f}{val_r2:<15.4f}\")\n",
        "\n",
        "            # Early stopping if loss is small enough\n",
        "            if train_loss <= 1e-4:\n",
        "                break\n",
        "\n",
        "        print(\"=\" * 65)\n",
        "        print(f\"Final Training Loss: {train_loss:.4f}, Final Training R²: {train_r2:.4f}\")\n",
        "        print(f\"Final Validation Loss: {val_loss:.4f}, Final Validation R²: {val_r2:.4f}\")\n",
        "\n",
        "        # After training, plot the metrics\n",
        "        self.plot_training_history(self.train_losses, self.val_losses, self.train_r2_scores, self.val_r2_scores)\n",
        "\n",
        "    def plot_training_history(self, train_losses, val_losses, train_r2_scores, val_r2_scores):\n",
        "        epochs_range = range(1, len(train_losses) + 1)\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss (MSE)\")\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.legend()\n",
        "\n",
        "        # R² Scores plot (Accuracy)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_range, train_r2_scores, label=\"Training R² Score\")\n",
        "        plt.plot(epochs_range, val_r2_scores, label=\"Validation R² Score\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"R² Score\")\n",
        "        plt.title(\"Training and Validation R² Score\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the neural network\n",
        "input_size = 5\n",
        "hidden_sizes = [7, 7, 7, 7, 7]  # Five hidden layers with 7 neurons each\n",
        "output_size = 1\n",
        "learning_rate = 0.01  # Reduced learning rate for stability\n",
        "l2_lambda = 0.001  # Regularization parameter\n",
        "epochs = 100 # Must be equal to k_max/2\n",
        "\n",
        "# Assuming predefined weights and biases (W, b) and data (X_train, y_train, X_val, y_val)\n",
        "nns = NeuralNetworkStochastic(weights, biases, learning_rate, l2_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model and visualize metrics\n",
        "nns.train(X_train_norm, y_train_norm, X_val_norm, y_val_norm, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the stored losses and accuracies\n",
        "train_losses = nn.train_losses  # List of training losses\n",
        "val_losses = nn.val_losses      # List of validation losses\n",
        "train_r2_scores = nn.train_r2_scores  # List of training R² scores\n",
        "val_r2_scores = nn.val_r2_scores      # List of validation R² scores\n",
        "\n",
        "# Print the final losses and R² scores\n",
        "print(\"Final Training Losses:\", train_losses[-1])\n",
        "print(\"Final Validation Losses:\", val_losses[-1])\n",
        "print(\"Final Training R² Scores:\", train_r2_scores[-1])\n",
        "print(\"Final Validation R² Scores:\", val_r2_scores[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the predictions for the training and validation sets\n",
        "y_train_pred_norm = nns.y_train_pred  # Predicted values for the training set\n",
        "y_val_pred_norm = nns.y_val_pred      # Predicted values for the validation set\n",
        "\n",
        "# Denormalize the predictions\n",
        "y_train_pred = denormalize(y_train_pred_norm, np.min(y_train), np.max(y_train))\n",
        "y_val_pred = denormalize(y_val_pred_norm, np.min(y_val), np.max(y_val))\n",
        "\n",
        "# Call the function to plot for training and validation\n",
        "plot_actual_vs_predicted(y_train, y_train_pred, title=\"Training: Actual vs Predicted\")\n",
        "plot_actual_vs_predicted(y_val, y_val_pred, title=\"Validation: Actual vs Predicted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of samples\n",
        "k_max = 20000\n",
        "t_test = np.linspace(0, k_max, k_max)  # Time variable\n",
        "\n",
        "# Generate sinusoidal input for u_test(k)\n",
        "frequency = 10  # Adjust the frequency as needed\n",
        "amplitude = 10    # Amplitude of the sine wave\n",
        "phase = 0         # Phase shift of the sine wave\n",
        "u_test = amplitude * np.sin(2 * np.pi * frequency * t_test + phase).astype(np.float32)\n",
        "\n",
        "# Initialize y_test(k) array with dtype float32\n",
        "y_test = np.zeros(k_max, dtype=np.float32)\n",
        "\n",
        "# Define y_test(k) calculations with dtype float32\n",
        "for k in range(1, k_max):\n",
        "    y_test[k] = np.float32(1 / (1 + (y_test[k-1])**2)) + np.float32(0.25 * u_test[k]) - np.float32(0.3 * u_test[k-1])\n",
        "\n",
        "# Shift arrays for y_test(k-1), y_test(k-2), u_test(k-1), u_test(k-2)\n",
        "y_k_1_test = np.zeros(k_max, dtype=np.float32)\n",
        "y_k_2_test = np.zeros(k_max, dtype=np.float32)\n",
        "u_k_1_test = np.zeros(k_max, dtype=np.float32)\n",
        "u_k_2_test = np.zeros(k_max, dtype=np.float32)\n",
        "\n",
        "# Shifted arrays for testing\n",
        "y_k_1_test[1:] = y_test[:-1]\n",
        "y_k_2_test[2:] = y_test[:-2]\n",
        "u_k_1_test[1:] = u_test[:-1]\n",
        "u_k_2_test[2:] = u_test[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = np.array([y_k_1_test, y_k_2_test, u_test, u_k_1_test, u_k_2_test]).T\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Normalize each component of X_test\n",
        "y_k_1_test_norm = normalize(y_k_1_test)\n",
        "y_k_2_test_norm = normalize(y_k_2_test)\n",
        "u_test_norm = normalize(u_test)\n",
        "u_k_1_test_norm = normalize(u_k_1_test)\n",
        "u_k_2_test_norm = normalize(u_k_2_test)\n",
        "y_test_norm = normalize(y_test)\n",
        "\n",
        "# Combine normalized inputs into X_test_norm\n",
        "X_test_norm = np.array([y_k_1_test_norm, y_k_2_test_norm, u_test_norm, u_k_1_test_norm, u_k_2_test_norm]).T\n",
        "\n",
        "y_pred_norm = nn.forward(X_test_norm)\n",
        "\n",
        "y_pred = denormalize(y_pred_norm, np.min(y_test), np.max(y_test))\n",
        "\n",
        "plot_actual_vs_predicted(y_test, y_pred, title=\"Test: Actual vs Predicted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
